# DEEP Vision Architectures: MiniVGG, ResNet & Vision Transformer on CIFAR-10 (CUDA Optimized)

---

## ğŸ“Œ Project Objective

Build, train, and compare multiple deep learning architectures on the CIFAR-10 dataset using PyTorch with CUDA acceleration. This repository showcases an evolutionary journey from classical CNNs to advanced Transformer-based vision models:

1. âœ… MiniVGG â€” Simple convolutional model  
2. âœ… MiniVGG Optimized â€” BatchNorm & LR Scheduling  
3. âœ… MiniResNet â€” Residual learning for deeper representation  
4. âœ… Vision Transformer (ViT) â€” Attention-based representation learning

---

## ğŸ“Š Dataset Details

- **Dataset**: CIFAR-10  
- **Images**: 60,000 color images of 10 categories (32Ã—32)  
- **Split**: 50,000 for training, 10,000 for testing  
- **Classes**: Airplane, Car, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck

---

## ğŸ§  Model Architectures

### ğŸ”¹ MiniVGG (Baseline CNN)
- 4 Conv layers â†’ MaxPooling  
- Fully Connected â†’ Dropout â†’ Output  
- âŒ No normalization  
- âŒ No scheduler  

### ğŸ”¹ MiniVGG Optimized
- Same layout as MiniVGG  
- âœ… Batch Normalization after each Conv  
- âœ… StepLR Scheduler (step=10, Î³=0.5)  
- âœ… More stable and higher accuracy  

### ğŸ”¹ MiniResNet
- 2 Residual Blocks with Conv-BN-ReLU  
- Initial Conv â†’ Residual Blocks â†’ Adaptive Pool â†’ FC  
- âœ… ReduceLROnPlateau Scheduler  
- âœ… Early Stopping  
- âœ… Better generalization & gradient flow  

### ğŸ”¹ Vision Transformer (ViT)
- Patch-based embedding using Conv2D  
- Learnable [CLS] token and positional embeddings  
- Transformer Encoder with 8 layers, 8 heads  
- MLP classification head  
- âœ… Warmup + Cosine Annealing LR Scheduler  
- âœ… RandAugment + Strong Regularization  
- âœ… Attention-based global learning

---

## ğŸ§ª Results Comparison

| Model               | Epochs | Final Val Acc | Final Test Acc | Best Val Epoch | Total Training Time |
|--------------------|--------|----------------|----------------|----------------|----------------------|
| MiniVGG            | 10     | 77.64%         | **77.36%**     | 10             | 2.84 min (pin=True)  |
| MiniVGG Optimized  | 20     | 79.36%         | **79.36%** âœ…   | 20             | 5.98 min             |
| MiniResNet         | 20     | 77.96%         | **78.05%**     | 20             | ~6.5 min             |
| Vision Transformer | 25     | 78.62%         | **78.62%**     | 25             | **1498.44 sec (~25 min)** â±ï¸ |

---

## ğŸ”§ Training Details

| Feature               | MiniVGG | MiniVGG Optimized | MiniResNet | Vision Transformer |
|-----------------------|---------|-------------------|------------|---------------------|
| BatchNorm             | âŒ      | âœ…                | âœ…         | âœ…                  |
| Scheduler             | âŒ      | StepLR            | ReduceLROnPlateau | Warmup + Cosine   |
| Early Stopping        | âŒ      | âŒ                | âœ…         | âŒ                  |
| Optimizer             | Adam    | Adam              | Adam       | AdamW               |
| Augmentation          | Basic   | Basic             | RandCrop + Flip | RandAugment + Flip |
| Epochs                | 10      | 20                | 20         | 25                  |
| Pin Memory            | âœ…      | âœ…                | âœ…         | âœ…                  |

---

## ğŸ§© Transformer Details (ViT)

| Parameter           | Value    |
|---------------------|----------|
| Patch Size          | 4Ã—4      |
| Embedding Dim       | 256      |
| Layers (Depth)      | 8        |
| Attention Heads     | 8        |
| MLP Hidden Dim      | 512      |
| Dropout             | 0.1      |
| LR Scheduler        | WarmupCosineAnnealingLR |
| LR                  | 3e-4     |
| RandAugment         | âœ… num_ops=2, magnitude=9 |
| Weight Decay        | 1e-4     |
| Final Accuracy      | **78.62%** ğŸ¯ |

---

## ğŸ“¦ Repository Structure

| File / Folder                | Description |
|-----------------------------|-------------|
| `MINI_VGG.ipynb`            | Baseline CNN |
| `Mini_VGG optimized.ipynb`  | CNN with BatchNorm + Scheduler |
| `Mini_ResNet.ipynb`         | Lightweight ResNet |
| `VisionTransformer.ipynb`   | Vision Transformer on CIFAR-10 |
| `README.md`                 | This full documentation |
| `./data`                    | CIFAR-10 dataset (downloaded automatically) |

---

## ğŸ”® Insights & Observations

- **MiniVGG Optimized** gave the best accuracy (79.36%) in shortest time.
- **ResNet** introduced better gradient flow via skip connections.
- **ViT** achieved 78.62% with attention and no convolutions after patching.
- Training ViT is **more resource-intensive** but demonstrates strong performance with enough tuning.
- âš ï¸ ViT requires **more data or augmentations** to outperform CNNs on small datasets.



â­ If this repo helped you, please **star it** and share your feedback!

